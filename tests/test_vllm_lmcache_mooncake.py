#!/usr/bin/env python3
"""
vLLM + LMCache + Mooncake ç¼“å­˜æ•ˆæœæµ‹è¯•è„šæœ¬

è¿™ä¸ªè„šæœ¬ç”¨äºæµ‹è¯• vLLM ä¸ LMCache + Mooncake é›†æˆåçš„ KV Cache ç¼“å­˜æ•ˆæœã€‚
æ”¯æŒå¤šç§æµ‹è¯•åœºæ™¯ï¼ŒåŒ…æ‹¬ï¼š
- Cold Start vs Cache Hit å¯¹æ¯”
- ä¸åŒå‰ç¼€é‡ç”¨ç‡æµ‹è¯•
- é•¿ä¸Šä¸‹æ–‡æµ‹è¯•
- å¹¶å‘æ€§èƒ½æµ‹è¯•
"""

import asyncio
import json
import time
import argparse
import yaml
from typing import List, Dict, Any, Optional
from dataclasses import dataclass, asdict
from datetime import datetime
import statistics
import sys

try:
    from openai import AsyncOpenAI
except ImportError:
    print("é”™è¯¯: éœ€è¦å®‰è£… openai åŒ…")
    print("è¿è¡Œ: pip install openai")
    sys.exit(1)


@dataclass
class RequestResult:
    """å•ä¸ªè¯·æ±‚çš„ç»“æœ"""
    request_id: int
    scenario: str
    round_num: int
    prompt_length: int
    success: bool
    elapsed_time: float
    ttft: Optional[float] = None
    tpot: Optional[float] = None
    output_tokens: int = 0
    error: Optional[str] = None
    timestamp: float = 0


@dataclass
class RoundStats:
    """ä¸€è½®æµ‹è¯•çš„ç»Ÿè®¡ç»“æœ"""
    scenario: str
    round_num: int
    total_requests: int
    success_requests: int
    failed_requests: int
    total_time: float
    avg_latency: float
    median_latency: float
    p90_latency: float
    p99_latency: float
    throughput: float
    avg_ttft: Optional[float] = None
    median_ttft: Optional[float] = None
    p90_ttft: Optional[float] = None
    avg_tpot: Optional[float] = None
    median_tpot: Optional[float] = None
    p90_tpot: Optional[float] = None


class VLLMCacheTestRunner:
    """æµ‹è¯•è¿è¡Œå™¨"""

    def __init__(self, config_path: str = "test_config.yaml"):
        """åˆå§‹åŒ–æµ‹è¯•è¿è¡Œå™¨"""
        with open(config_path, 'r', encoding='utf-8') as f:
            self.config = yaml.safe_load(f)

        self.proxy_url = self.config['proxy']['url']
        self.model_name = self.config['model']['name']
        self.results: List[RequestResult] = []
        self.round_stats: List[RoundStats] = []

        print(f"âœ“ æµ‹è¯•é…ç½®åŠ è½½æˆåŠŸ")
        print(f"  Proxy URL: {self.proxy_url}")
        print(f"  Model: {self.model_name}")

    def generate_prompts(self, scenario: str) -> List[str]:
        """ç”Ÿæˆæµ‹è¯•æç¤ºè¯"""
        scenario_config = self.config['test_scenarios'][scenario]
        num_requests = scenario_config['num_requests']
        prompt_template = scenario_config['prompt_template']
        reuse_ratio = scenario_config.get('reuse_ratio', 0)

        prompts = []

        if scenario == 'high_reuse':
            # é«˜é‡ç”¨ç‡ï¼šæ‰€æœ‰è¯·æ±‚ä½¿ç”¨ç›¸åŒçš„é•¿å‰ç¼€
            base_prefix = prompt_template.format(i=0)
            for i in range(num_requests):
                prompts.append(base_prefix + f"\nè¯·æ±‚ID: {i}")

        elif scenario == 'medium_reuse':
            # ä¸­ç­‰é‡ç”¨ç‡ï¼šéƒ¨åˆ†è¯·æ±‚å…±äº«å‰ç¼€
            num_groups = max(1, int(num_requests * (1 - reuse_ratio)))
            for i in range(num_requests):
                group_id = i % num_groups
                prompts.append(prompt_template.format(i=group_id, req=i))

        elif scenario == 'low_reuse':
            # ä½é‡ç”¨ç‡ï¼šæ¯ä¸ªè¯·æ±‚éƒ½æ˜¯å”¯ä¸€çš„
            for i in range(num_requests):
                prompts.append(prompt_template.format(i=i))

        elif scenario == 'long_context':
            # é•¿ä¸Šä¸‹æ–‡æµ‹è¯•
            context_length = scenario_config.get('context_length', 8192)
            base_text = "è¿™æ˜¯ä¸€æ®µç”¨äºæµ‹è¯•é•¿ä¸Šä¸‹æ–‡çš„æ–‡æœ¬ã€‚" * (context_length // 50)
            for i in range(num_requests):
                prompts.append(f"{base_text}\n\né—®é¢˜{i}: {prompt_template.format(i=i)}")

        else:
            raise ValueError(f"æœªçŸ¥çš„æµ‹è¯•åœºæ™¯: {scenario}")

        return prompts

    async def send_request(
        self,
        client: AsyncOpenAI,
        prompt: str,
        request_id: int,
        scenario: str,
        round_num: int
    ) -> RequestResult:
        """å‘é€å•ä¸ªè¯·æ±‚å¹¶è®°å½•ç»“æœ"""
        start_time = time.time()
        timestamp = datetime.now().timestamp()

        try:
            response = await client.chat.completions.create(
                model=self.model_name,
                messages=[{"role": "user", "content": prompt}],
                max_tokens=self.config['model']['max_tokens'],
                temperature=self.config['model']['temperature'],
            )

            elapsed = time.time() - start_time

            # æå–è¾“å‡º tokens æ•°é‡
            output_tokens = 0
            if hasattr(response, 'usage') and response.usage:
                output_tokens = response.usage.completion_tokens

            result = RequestResult(
                request_id=request_id,
                scenario=scenario,
                round_num=round_num,
                prompt_length=len(prompt),
                success=True,
                elapsed_time=elapsed,
                output_tokens=output_tokens,
                timestamp=timestamp
            )

            return result

        except Exception as e:
            elapsed = time.time() - start_time
            return RequestResult(
                request_id=request_id,
                scenario=scenario,
                round_num=round_num,
                prompt_length=len(prompt),
                success=False,
                elapsed_time=elapsed,
                error=str(e),
                timestamp=timestamp
            )

    async def run_scenario(
        self,
        scenario: str,
        num_rounds: int = 2,
        concurrency: Optional[int] = None
    ) -> List[RoundStats]:
        """è¿è¡Œå•ä¸ªæµ‹è¯•åœºæ™¯"""
        print(f"\n{'='*80}")
        print(f"ğŸ§ª æµ‹è¯•åœºæ™¯: {scenario}")
        print(f"{'='*80}")

        client = AsyncOpenAI(base_url=self.proxy_url, api_key="dummy")
        prompts = self.generate_prompts(scenario)

        print(f"ç”Ÿæˆäº† {len(prompts)} ä¸ªæµ‹è¯•è¯·æ±‚")
        print(f"å°†è¿è¡Œ {num_rounds} è½®æµ‹è¯•")

        if concurrency:
            print(f"å¹¶å‘é™åˆ¶: {concurrency}")

        scenario_stats = []

        for round_num in range(num_rounds):
            print(f"\n{'â”€'*80}")
            print(f"ğŸ“Š Round {round_num + 1}/{num_rounds}")
            print(f"{'â”€'*80}")

            round_start = time.time()

            # åˆ›å»ºä»»åŠ¡
            tasks = [
                self.send_request(client, prompt, i, scenario, round_num + 1)
                for i, prompt in enumerate(prompts)
            ]

            # æ ¹æ®å¹¶å‘é™åˆ¶æ‰§è¡Œä»»åŠ¡
            if concurrency:
                results = []
                for i in range(0, len(tasks), concurrency):
                    batch = tasks[i:i+concurrency]
                    batch_results = await asyncio.gather(*batch)
                    results.extend(batch_results)
                    print(f"  å·²å®Œæˆ {min(i+concurrency, len(tasks))}/{len(tasks)} ä¸ªè¯·æ±‚...")
            else:
                results = await asyncio.gather(*tasks)

            round_elapsed = time.time() - round_start

            # ä¿å­˜ç»“æœ
            self.results.extend(results)

            # è®¡ç®—ç»Ÿè®¡æ•°æ®
            stats = self._calculate_stats(results, scenario, round_num + 1, round_elapsed)
            scenario_stats.append(stats)
            self.round_stats.append(stats)

            # æ‰“å°ç»“æœ
            self._print_round_stats(stats, round_num)

            # å¦‚æœæœ‰å¤šè½®ï¼Œæ¯”è¾ƒæ€§èƒ½æå‡
            if round_num > 0:
                self._print_improvement(scenario_stats[0], stats)

            # ç­‰å¾…ä¸€æ®µæ—¶é—´å†è¿›è¡Œä¸‹ä¸€è½®
            if round_num < num_rounds - 1:
                wait_time = self.config.get('round_wait_seconds', 5)
                print(f"\nç­‰å¾… {wait_time} ç§’åå¼€å§‹ä¸‹ä¸€è½®...")
                await asyncio.sleep(wait_time)

        return scenario_stats

    def _calculate_stats(
        self,
        results: List[RequestResult],
        scenario: str,
        round_num: int,
        total_time: float
    ) -> RoundStats:
        """è®¡ç®—ç»Ÿè®¡æ•°æ®"""
        success_results = [r for r in results if r.success]
        failed_results = [r for r in results if not r.success]

        latencies = [r.elapsed_time for r in success_results]

        if not latencies:
            # æ‰€æœ‰è¯·æ±‚éƒ½å¤±è´¥äº†
            return RoundStats(
                scenario=scenario,
                round_num=round_num,
                total_requests=len(results),
                success_requests=0,
                failed_requests=len(failed_results),
                total_time=total_time,
                avg_latency=0,
                median_latency=0,
                p90_latency=0,
                p99_latency=0,
                throughput=0
            )

        latencies.sort()

        return RoundStats(
            scenario=scenario,
            round_num=round_num,
            total_requests=len(results),
            success_requests=len(success_results),
            failed_requests=len(failed_results),
            total_time=total_time,
            avg_latency=statistics.mean(latencies),
            median_latency=statistics.median(latencies),
            p90_latency=latencies[int(len(latencies) * 0.9)] if latencies else 0,
            p99_latency=latencies[int(len(latencies) * 0.99)] if len(latencies) > 1 else latencies[0],
            throughput=len(success_results) / total_time if total_time > 0 else 0
        )

    def _print_round_stats(self, stats: RoundStats, round_num: int):
        """æ‰“å°å•è½®ç»Ÿè®¡ç»“æœ"""
        print(f"\nğŸ“ˆ ç»Ÿè®¡ç»“æœ:")
        print(f"  æ€»è¯·æ±‚æ•°:     {stats.total_requests}")
        print(f"  æˆåŠŸè¯·æ±‚:     {stats.success_requests}")
        print(f"  å¤±è´¥è¯·æ±‚:     {stats.failed_requests}")
        print(f"  æ€»è€—æ—¶:       {stats.total_time:.2f}s")
        print(f"\n  å¹³å‡å»¶è¿Ÿ:     {stats.avg_latency*1000:.2f}ms")
        print(f"  ä¸­ä½æ•°å»¶è¿Ÿ:   {stats.median_latency*1000:.2f}ms")
        print(f"  P90 å»¶è¿Ÿ:     {stats.p90_latency*1000:.2f}ms")
        print(f"  P99 å»¶è¿Ÿ:     {stats.p99_latency*1000:.2f}ms")
        print(f"\n  ååé‡:       {stats.throughput:.2f} req/s")

    def _print_improvement(self, baseline: RoundStats, current: RoundStats):
        """æ‰“å°æ€§èƒ½æå‡"""
        if baseline.avg_latency == 0:
            return

        latency_improvement = (1 - current.avg_latency / baseline.avg_latency) * 100
        throughput_improvement = (current.throughput / baseline.throughput - 1) * 100

        print(f"\nğŸ¯ ç¼“å­˜æ•ˆæœ (ç›¸æ¯” Round 1):")
        print(f"  å»¶è¿Ÿé™ä½:     {latency_improvement:+.1f}%")
        print(f"  ååé‡æå‡:   {throughput_improvement:+.1f}%")

        if latency_improvement > 50:
            print(f"  âœ… ç¼“å­˜æ•ˆæœæ˜¾è‘—ï¼")
        elif latency_improvement > 20:
            print(f"  âš ï¸  ç¼“å­˜æ•ˆæœä¸€èˆ¬")
        else:
            print(f"  âŒ ç¼“å­˜æ•ˆæœä¸æ˜æ˜¾")

    def save_results(self, output_dir: str = "test_results"):
        """ä¿å­˜æµ‹è¯•ç»“æœ"""
        import os
        os.makedirs(output_dir, exist_ok=True)

        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

        # ä¿å­˜è¯¦ç»†ç»“æœ
        results_file = f"{output_dir}/results_{timestamp}.json"
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump(
                [asdict(r) for r in self.results],
                f,
                indent=2,
                ensure_ascii=False
            )

        # ä¿å­˜ç»Ÿè®¡æ‘˜è¦
        stats_file = f"{output_dir}/stats_{timestamp}.json"
        with open(stats_file, 'w', encoding='utf-8') as f:
            json.dump(
                [asdict(s) for s in self.round_stats],
                f,
                indent=2,
                ensure_ascii=False
            )

        print(f"\nğŸ’¾ ç»“æœå·²ä¿å­˜:")
        print(f"  è¯¦ç»†ç»“æœ: {results_file}")
        print(f"  ç»Ÿè®¡æ‘˜è¦: {stats_file}")

        return results_file, stats_file


async def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description="vLLM + LMCache + Mooncake ç¼“å­˜æ•ˆæœæµ‹è¯•"
    )
    parser.add_argument(
        '--config',
        type=str,
        default='test_config.yaml',
        help='é…ç½®æ–‡ä»¶è·¯å¾„'
    )
    parser.add_argument(
        '--scenarios',
        type=str,
        nargs='+',
        default=['high_reuse', 'medium_reuse', 'low_reuse'],
        help='è¦è¿è¡Œçš„æµ‹è¯•åœºæ™¯'
    )
    parser.add_argument(
        '--rounds',
        type=int,
        default=2,
        help='æ¯ä¸ªåœºæ™¯è¿è¡Œçš„è½®æ•°'
    )
    parser.add_argument(
        '--concurrency',
        type=int,
        help='å¹¶å‘è¯·æ±‚æ•°é™åˆ¶'
    )
    parser.add_argument(
        '--output-dir',
        type=str,
        default='test_results',
        help='ç»“æœè¾“å‡ºç›®å½•'
    )

    args = parser.parse_args()

    print("="*80)
    print("ğŸš€ vLLM + LMCache + Mooncake ç¼“å­˜æ•ˆæœæµ‹è¯•")
    print("="*80)
    print(f"å¼€å§‹æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")

    try:
        runner = VLLMCacheTestRunner(args.config)

        # è¿è¡Œæ‰€æœ‰æµ‹è¯•åœºæ™¯
        for scenario in args.scenarios:
            await runner.run_scenario(
                scenario,
                num_rounds=args.rounds,
                concurrency=args.concurrency
            )

        # ä¿å­˜ç»“æœ
        results_file, stats_file = runner.save_results(args.output_dir)

        print(f"\n{'='*80}")
        print("âœ… æ‰€æœ‰æµ‹è¯•å®Œæˆ!")
        print(f"{'='*80}")
        print(f"ç»“æŸæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")

        # ç”ŸæˆæŠ¥å‘Šæç¤º
        print(f"\nğŸ“Š ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š:")
        print(f"  python3 generate_report.py --stats {stats_file}")

    except Exception as e:
        print(f"\nâŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    asyncio.run(main())
