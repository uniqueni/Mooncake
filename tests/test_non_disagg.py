#!/usr/bin/env python3
"""
é PD åˆ†ç¦»æ¨¡å¼æµ‹è¯•è„šæœ¬

æµ‹è¯•ä¼ ç»Ÿ vLLM éƒ¨ç½²æ¨¡å¼ï¼ˆå•æœåŠ¡ï¼‰ï¼Œé‡ç‚¹æµ‹é‡ï¼š
- TTFT (Time to First Token)
- TPOT (Time per Output Token)
- ååé‡
- ç«¯åˆ°ç«¯å»¶è¿Ÿ
- ç¼“å­˜æ•ˆæœï¼ˆå¦‚æœå¯ç”¨ï¼‰
"""

import asyncio
import json
import time
import argparse
import yaml
from typing import List, Dict, Any, Optional
from dataclasses import dataclass, asdict
from datetime import datetime
import statistics
import sys

try:
    from openai import AsyncOpenAI
except ImportError:
    print("é”™è¯¯: éœ€è¦å®‰è£… openai åŒ…")
    print("è¿è¡Œ: pip install openai")
    sys.exit(1)


@dataclass
class RequestMetrics:
    """å•ä¸ªè¯·æ±‚çš„æ€§èƒ½æŒ‡æ ‡"""
    request_id: int
    scenario: str
    round_num: int

    # åŸºæœ¬ä¿¡æ¯
    prompt_length: int  # è¾“å…¥ tokens
    output_length: int  # è¾“å‡º tokens

    # æ—¶é—´æŒ‡æ ‡
    ttft: Optional[float] = None  # Time to First Token (ç§’)
    tpot: Optional[float] = None  # Time per Output Token (ç§’)
    e2e_latency: float = 0.0  # ç«¯åˆ°ç«¯å»¶è¿Ÿ (ç§’)

    # çŠ¶æ€
    success: bool = True
    error: Optional[str] = None
    timestamp: float = 0.0


@dataclass
class ScenarioStats:
    """åœºæ™¯ç»Ÿè®¡æ•°æ®"""
    scenario: str
    round_num: int
    total_requests: int
    success_requests: int
    failed_requests: int
    total_time: float

    # TTFT ç»Ÿè®¡
    avg_ttft: float = 0.0
    median_ttft: float = 0.0
    p90_ttft: float = 0.0
    p99_ttft: float = 0.0

    # TPOT ç»Ÿè®¡
    avg_tpot: float = 0.0
    median_tpot: float = 0.0
    p90_tpot: float = 0.0

    # å»¶è¿Ÿç»Ÿè®¡
    avg_latency: float = 0.0
    median_latency: float = 0.0
    p90_latency: float = 0.0
    p99_latency: float = 0.0

    # ååé‡
    request_throughput: float = 0.0  # req/s
    token_throughput: float = 0.0  # tokens/s
    output_token_throughput: float = 0.0  # output tokens/s


class NonDisaggTestRunner:
    """é PD åˆ†ç¦»æ¨¡å¼æµ‹è¯•è¿è¡Œå™¨"""

    def __init__(self, config_path: str = "test_config_large_models.yaml"):
        """åˆå§‹åŒ–æµ‹è¯•è¿è¡Œå™¨"""
        with open(config_path, 'r', encoding='utf-8') as f:
            self.config = yaml.safe_load(f)

        # è·å–æ´»åŠ¨æ¨¡å‹é…ç½®
        active_model = self.config['active_model']
        self.model_config = self.config['models'][active_model]
        self.model_name = self.model_config['name']

        # è·å–é PD åˆ†ç¦»éƒ¨ç½²é…ç½®
        deploy_config = self.config['deployment_modes']['non_disaggregated']
        if not deploy_config['enabled']:
            raise ValueError("é PD åˆ†ç¦»æ¨¡å¼æœªå¯ç”¨ï¼Œè¯·æ£€æŸ¥é…ç½®æ–‡ä»¶")

        server_config = deploy_config['server']
        self.server_url = f"http://{server_config['host']}:{server_config['port']}/v1"

        self.results: List[RequestMetrics] = []
        self.scenario_stats: List[ScenarioStats] = []

        print(f"âœ“ é PD åˆ†ç¦»æ¨¡å¼æµ‹è¯•é…ç½®åŠ è½½æˆåŠŸ")
        print(f"  æ¨¡å‹: {self.model_name}")
        print(f"  æœåŠ¡å™¨: {self.server_url}")

    def _generate_long_document(self, length: int = 16384) -> str:
        """ç”ŸæˆæŒ‡å®šé•¿åº¦çš„é•¿æ–‡æ¡£ï¼ˆä¼°ç®— tokensï¼‰"""
        # ç®€åŒ–ï¼šå‡è®¾æ¯ä¸ªå•è¯çº¦ 1.3 tokens
        words_needed = int(length / 1.3)

        # ç”ŸæˆæŠ€æœ¯æ–‡æ¡£æ ·æœ¬
        base_text = """
        # Mooncake: A KVCache-centric Disaggregated Architecture for LLM Serving

        ## Introduction
        Large Language Models (LLMs) have revolutionized natural language processing tasks.
        However, serving these models efficiently at scale remains a significant challenge.
        Traditional architectures often suffer from low GPU utilization and high latency.

        ## Architecture Overview
        Mooncake introduces a disaggregated architecture that separates the prefill and decode
        phases into different clusters. This design enables better resource utilization and
        improves overall system throughput. The key innovation is a KVCache-centric scheduler
        that balances throughput maximization with latency SLO requirements.

        ## Technical Details
        The system consists of several core components:
        1. Transfer Engine: High-performance data transfer layer supporting RDMA and TCP
        2. Mooncake Store: Distributed KVCache storage across multiple nodes
        3. Scheduler: Intelligent request routing and cache management
        4. Metadata Service: Centralized coordination using etcd or HTTP endpoints

        ## Performance Results
        Extensive benchmarks show that Mooncake achieves:
        - Up to 525% increase in throughput compared to baseline methods
        - 60-70% reduction in Time to First Token (TTFT) with cache hits
        - Efficient handling of long-context scenarios (128k+ tokens)
        - Scalability to thousands of GPUs in production environments
        """

        # é‡å¤æ–‡æœ¬ç›´åˆ°è¾¾åˆ°æ‰€éœ€é•¿åº¦
        repeated_text = (base_text * (words_needed // len(base_text.split()) + 1))
        words = repeated_text.split()[:words_needed]

        return ' '.join(words)

    def generate_prompts(self, scenario: str) -> List[Dict[str, Any]]:
        """ç”Ÿæˆæµ‹è¯•æç¤ºè¯"""
        scenario_config = self.config['test_scenarios'][scenario]
        num_requests = scenario_config['num_requests']

        prompts = []

        if scenario == 'long_context_high_reuse':
            # ç”Ÿæˆé•¿æ–‡æ¡£
            long_doc = self._generate_long_document(
                scenario_config['context_length']
            )

            questions = scenario_config['questions']

            for i in range(num_requests):
                question = questions[i % len(questions)]
                prompt = f"{long_doc}\n\nåŸºäºä¸Šè¿°æ–‡æ¡£ï¼Œè¯·å›ç­”é—®é¢˜ #{i}: {question}"
                prompts.append({
                    'prompt': prompt,
                    'estimated_tokens': scenario_config['context_length']
                })

        elif scenario == 'multi_turn_conversation':
            # æ¨¡æ‹Ÿå¤šè½®å¯¹è¯ï¼Œé€æ­¥ç´¯ç§¯ä¸Šä¸‹æ–‡
            conversation_history = ""
            turns = scenario_config['conversation_turns']

            for i in range(num_requests):
                turn = turns[i % len(turns)]
                conversation_history += f"\nUser: {turn}\nAssistant: [Previous response]\n"

                prompt = f"{conversation_history}\nUser: {turn}\nAssistant:"
                prompts.append({
                    'prompt': prompt,
                    'estimated_tokens': len(conversation_history.split()) * 1.3
                })

        elif scenario == 'batch_processing':
            # æ‰¹é‡å¤„ç†ï¼Œå…±äº«å¤§é‡æŒ‡ä»¤
            instruction = scenario_config['prompt_template'].split('Text to translate')[0]

            for i in range(num_requests):
                text = f"Sample text for translation task {i}. This is a technical document about distributed systems and high-performance computing."
                prompt = instruction + f"\n\nText to translate #{i}:\n{text}"
                prompts.append({
                    'prompt': prompt,
                    'estimated_tokens': len(instruction.split()) * 1.3 + 50
                })

        elif scenario == 'cold_start':
            # å†·å¯åŠ¨ï¼Œæ¯ä¸ªè¯·æ±‚éƒ½æ˜¯å”¯ä¸€çš„
            for i in range(num_requests):
                unique_content = f"This is a unique question about topic {i * 137 % 1000}. " * 50
                prompt = f"Question #{i}: {unique_content}"
                prompts.append({
                    'prompt': prompt,
                    'estimated_tokens': 700
                })

        else:
            raise ValueError(f"æœªçŸ¥çš„æµ‹è¯•åœºæ™¯: {scenario}")

        return prompts

    async def send_request(
        self,
        client: AsyncOpenAI,
        prompt_data: Dict[str, Any],
        request_id: int,
        scenario: str,
        round_num: int
    ) -> RequestMetrics:
        """å‘é€å•ä¸ªè¯·æ±‚å¹¶æµ‹é‡æ€§èƒ½æŒ‡æ ‡"""
        prompt = prompt_data['prompt']
        estimated_tokens = prompt_data['estimated_tokens']

        start_time = time.time()
        timestamp = datetime.now().timestamp()

        try:
            # ä½¿ç”¨æµå¼è¾“å‡ºä»¥æµ‹é‡ TTFT
            first_token_time = None
            output_tokens = 0
            total_output_time = 0

            stream = await client.chat.completions.create(
                model=self.model_name,
                messages=[{"role": "user", "content": prompt}],
                max_tokens=self.model_config['max_tokens'],
                temperature=self.model_config['temperature'],
                stream=True,
            )

            async for chunk in stream:
                if first_token_time is None:
                    first_token_time = time.time()

                if chunk.choices and chunk.choices[0].delta.content:
                    output_tokens += 1

            end_time = time.time()

            # è®¡ç®—æŒ‡æ ‡
            e2e_latency = end_time - start_time
            ttft = (first_token_time - start_time) if first_token_time else None

            total_output_time = end_time - first_token_time if first_token_time else e2e_latency
            tpot = (total_output_time / output_tokens) if output_tokens > 0 else None

            return RequestMetrics(
                request_id=request_id,
                scenario=scenario,
                round_num=round_num,
                prompt_length=int(estimated_tokens),
                output_length=output_tokens,
                ttft=ttft,
                tpot=tpot,
                e2e_latency=e2e_latency,
                success=True,
                timestamp=timestamp
            )

        except Exception as e:
            end_time = time.time()
            return RequestMetrics(
                request_id=request_id,
                scenario=scenario,
                round_num=round_num,
                prompt_length=int(estimated_tokens),
                output_length=0,
                e2e_latency=end_time - start_time,
                success=False,
                error=str(e),
                timestamp=timestamp
            )

    async def run_scenario(
        self,
        scenario: str,
        num_rounds: int = 2,
        concurrency: Optional[int] = None
    ) -> List[ScenarioStats]:
        """è¿è¡Œå•ä¸ªæµ‹è¯•åœºæ™¯"""
        print(f"\n{'='*80}")
        print(f"ğŸ§ª æµ‹è¯•åœºæ™¯: {scenario}")
        print(f"   {self.config['test_scenarios'][scenario]['description']}")
        print(f"{'='*80}")

        client = AsyncOpenAI(base_url=self.server_url, api_key="dummy")
        prompts = self.generate_prompts(scenario)

        print(f"ç”Ÿæˆäº† {len(prompts)} ä¸ªæµ‹è¯•è¯·æ±‚")
        print(f"å°†è¿è¡Œ {num_rounds} è½®æµ‹è¯•ï¼ˆç¬¬1è½®: Cold Start, ç¬¬2è½®: Cache Hitï¼‰")

        scenario_stats = []

        for round_num in range(num_rounds):
            print(f"\n{'â”€'*80}")
            print(f"ğŸ“Š Round {round_num + 1}/{num_rounds}")
            print(f"{'â”€'*80}")

            round_start = time.time()

            # åˆ›å»ºä»»åŠ¡
            tasks = [
                self.send_request(client, prompt, i, scenario, round_num + 1)
                for i, prompt in enumerate(prompts)
            ]

            # æ ¹æ®å¹¶å‘é™åˆ¶æ‰§è¡Œ
            if concurrency:
                results = []
                for i in range(0, len(tasks), concurrency):
                    batch = tasks[i:i+concurrency]
                    batch_results = await asyncio.gather(*batch)
                    results.extend(batch_results)
                    print(f"  å·²å®Œæˆ {min(i+concurrency, len(tasks))}/{len(tasks)} ä¸ªè¯·æ±‚...")
            else:
                results = await asyncio.gather(*tasks)

            round_elapsed = time.time() - round_start

            # ä¿å­˜ç»“æœ
            self.results.extend(results)

            # è®¡ç®—ç»Ÿè®¡
            stats = self._calculate_stats(results, scenario, round_num + 1, round_elapsed)
            scenario_stats.append(stats)
            self.scenario_stats.append(stats)

            # æ‰“å°ç»“æœ
            self._print_stats(stats, round_num)

            # å¯¹æ¯”æ€§èƒ½æå‡
            if round_num > 0:
                self._print_improvement(scenario_stats[0], stats)

            # ç­‰å¾…ä¸‹ä¸€è½®
            if round_num < num_rounds - 1:
                wait_time = self.config['performance_test'].get('rounds', 5)
                print(f"\nç­‰å¾… {wait_time} ç§’åå¼€å§‹ä¸‹ä¸€è½®...")
                await asyncio.sleep(wait_time)

        return scenario_stats

    def _calculate_stats(
        self,
        results: List[RequestMetrics],
        scenario: str,
        round_num: int,
        total_time: float
    ) -> ScenarioStats:
        """è®¡ç®—ç»Ÿè®¡æ•°æ®"""
        success_results = [r for r in results if r.success]

        if not success_results:
            return ScenarioStats(
                scenario=scenario,
                round_num=round_num,
                total_requests=len(results),
                success_requests=0,
                failed_requests=len(results),
                total_time=total_time
            )

        # TTFT ç»Ÿè®¡
        ttfts = [r.ttft for r in success_results if r.ttft is not None]
        ttfts.sort()

        # TPOT ç»Ÿè®¡
        tpots = [r.tpot for r in success_results if r.tpot is not None]
        tpots.sort()

        # å»¶è¿Ÿç»Ÿè®¡
        latencies = [r.e2e_latency for r in success_results]
        latencies.sort()

        # Token ååé‡
        total_input_tokens = sum(r.prompt_length for r in success_results)
        total_output_tokens = sum(r.output_length for r in success_results)
        total_tokens = total_input_tokens + total_output_tokens

        return ScenarioStats(
            scenario=scenario,
            round_num=round_num,
            total_requests=len(results),
            success_requests=len(success_results),
            failed_requests=len(results) - len(success_results),
            total_time=total_time,

            avg_ttft=statistics.mean(ttfts) if ttfts else 0,
            median_ttft=statistics.median(ttfts) if ttfts else 0,
            p90_ttft=ttfts[int(len(ttfts) * 0.9)] if ttfts else 0,
            p99_ttft=ttfts[int(len(ttfts) * 0.99)] if len(ttfts) > 1 else (ttfts[0] if ttfts else 0),

            avg_tpot=statistics.mean(tpots) if tpots else 0,
            median_tpot=statistics.median(tpots) if tpots else 0,
            p90_tpot=tpots[int(len(tpots) * 0.9)] if tpots else 0,

            avg_latency=statistics.mean(latencies) if latencies else 0,
            median_latency=statistics.median(latencies) if latencies else 0,
            p90_latency=latencies[int(len(latencies) * 0.9)] if latencies else 0,
            p99_latency=latencies[int(len(latencies) * 0.99)] if len(latencies) > 1 else (latencies[0] if latencies else 0),

            request_throughput=len(success_results) / total_time if total_time > 0 else 0,
            token_throughput=total_tokens / total_time if total_time > 0 else 0,
            output_token_throughput=total_output_tokens / total_time if total_time > 0 else 0
        )

    def _print_stats(self, stats: ScenarioStats, round_num: int):
        """æ‰“å°ç»Ÿè®¡ç»“æœ"""
        print(f"\nğŸ“ˆ ç»Ÿè®¡ç»“æœ:")
        print(f"  æ€»è¯·æ±‚æ•°:       {stats.total_requests}")
        print(f"  æˆåŠŸè¯·æ±‚:       {stats.success_requests}")
        print(f"  å¤±è´¥è¯·æ±‚:       {stats.failed_requests}")
        print(f"  æ€»è€—æ—¶:         {stats.total_time:.2f}s")

        print(f"\nâ±ï¸  TTFT (Time to First Token):")
        print(f"  å¹³å‡:           {stats.avg_ttft*1000:.2f}ms")
        print(f"  ä¸­ä½æ•°:         {stats.median_ttft*1000:.2f}ms")
        print(f"  P90:            {stats.p90_ttft*1000:.2f}ms")
        print(f"  P99:            {stats.p99_ttft*1000:.2f}ms")

        print(f"\nâš¡ TPOT (Time per Output Token):")
        print(f"  å¹³å‡:           {stats.avg_tpot*1000:.2f}ms")
        print(f"  ä¸­ä½æ•°:         {stats.median_tpot*1000:.2f}ms")
        print(f"  P90:            {stats.p90_tpot*1000:.2f}ms")

        print(f"\nğŸ• ç«¯åˆ°ç«¯å»¶è¿Ÿ:")
        print(f"  å¹³å‡:           {stats.avg_latency*1000:.2f}ms")
        print(f"  ä¸­ä½æ•°:         {stats.median_latency*1000:.2f}ms")
        print(f"  P90:            {stats.p90_latency*1000:.2f}ms")
        print(f"  P99:            {stats.p99_latency*1000:.2f}ms")

        print(f"\nğŸš€ ååé‡:")
        print(f"  è¯·æ±‚ååé‡:     {stats.request_throughput:.2f} req/s")
        print(f"  Token ååé‡:   {stats.token_throughput:.2f} tokens/s")
        print(f"  è¾“å‡º Token:     {stats.output_token_throughput:.2f} tokens/s")

    def _print_improvement(self, baseline: ScenarioStats, current: ScenarioStats):
        """æ‰“å°æ€§èƒ½æå‡"""
        print(f"\nğŸ¯ ç¼“å­˜æ•ˆæœ (ç›¸æ¯” Round 1):")

        if baseline.avg_ttft > 0:
            ttft_improvement = (1 - current.avg_ttft / baseline.avg_ttft) * 100
            print(f"  TTFT é™ä½:      {ttft_improvement:+.1f}%")

        if baseline.avg_latency > 0:
            latency_improvement = (1 - current.avg_latency / baseline.avg_latency) * 100
            print(f"  å»¶è¿Ÿé™ä½:       {latency_improvement:+.1f}%")

        if baseline.request_throughput > 0:
            throughput_improvement = (current.request_throughput / baseline.request_throughput - 1) * 100
            print(f"  ååé‡æå‡:     {throughput_improvement:+.1f}%")

        # è¯„ä»·
        if baseline.avg_ttft > 0:
            if ttft_improvement > 60:
                print(f"  âœ… ç¼“å­˜æ•ˆæœä¼˜ç§€ï¼")
            elif ttft_improvement > 30:
                print(f"  âš ï¸  ç¼“å­˜æ•ˆæœä¸€èˆ¬")
            else:
                print(f"  âŒ ç¼“å­˜æ•ˆæœä¸æ˜æ˜¾")

    def save_results(self, output_dir: str = "test_results"):
        """ä¿å­˜æµ‹è¯•ç»“æœ"""
        import os
        os.makedirs(output_dir, exist_ok=True)

        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        model_name = self.config['active_model']

        # ä¿å­˜è¯¦ç»†ç»“æœ
        results_file = f"{output_dir}/non_disagg_{model_name}_results_{timestamp}.json"
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump(
                [asdict(r) for r in self.results],
                f,
                indent=2,
                ensure_ascii=False
            )

        # ä¿å­˜ç»Ÿè®¡æ‘˜è¦
        stats_file = f"{output_dir}/non_disagg_{model_name}_stats_{timestamp}.json"
        with open(stats_file, 'w', encoding='utf-8') as f:
            json.dump(
                [asdict(s) for s in self.scenario_stats],
                f,
                indent=2,
                ensure_ascii=False
            )

        print(f"\nğŸ’¾ ç»“æœå·²ä¿å­˜:")
        print(f"  è¯¦ç»†ç»“æœ: {results_file}")
        print(f"  ç»Ÿè®¡æ‘˜è¦: {stats_file}")

        return results_file, stats_file


async def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="é PD åˆ†ç¦»æ¨¡å¼ç¼“å­˜æµ‹è¯•")
    parser.add_argument('--config', type=str, default='test_config_large_models.yaml')
    parser.add_argument('--scenarios', type=str, nargs='+',
                       default=['long_context_high_reuse', 'multi_turn_conversation'])
    parser.add_argument('--rounds', type=int, default=2)
    parser.add_argument('--concurrency', type=int, help='å¹¶å‘é™åˆ¶')
    parser.add_argument('--output-dir', type=str, default='test_results')

    args = parser.parse_args()

    print("="*80)
    print("ğŸš€ vLLM é PD åˆ†ç¦»æ¨¡å¼ - ç¼“å­˜æ•ˆæœæµ‹è¯•")
    print("="*80)
    print(f"å¼€å§‹æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")

    try:
        runner = NonDisaggTestRunner(args.config)

        for scenario in args.scenarios:
            await runner.run_scenario(scenario, args.rounds, args.concurrency)

        runner.save_results(args.output_dir)

        print(f"\n{'='*80}")
        print("âœ… æ‰€æœ‰æµ‹è¯•å®Œæˆ!")
        print(f"{'='*80}")

    except Exception as e:
        print(f"\nâŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    asyncio.run(main())
