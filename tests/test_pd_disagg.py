#!/usr/bin/env python3
"""
PD åˆ†ç¦»æ¨¡å¼æµ‹è¯•è„šæœ¬

æµ‹è¯• Prefill-Decode åˆ†ç¦»æ¶æ„ + LMCache + Mooncakeï¼Œé‡ç‚¹æµ‹é‡ï¼š
- TTFT (Time to First Token) - åŒ…æ‹¬ Prefill æ—¶é—´
- TPOT (Time per Output Token) - Decode é˜¶æ®µ
- KV Cache ä¼ è¾“æ—¶é—´
- ååé‡å’Œå»¶è¿Ÿ
- ç¼“å­˜å‘½ä¸­æ•ˆæœ
"""

import asyncio
import json
import time
import argparse
import yaml
from typing import List, Dict, Any, Optional
from dataclasses import dataclass, asdict
from datetime import datetime
import statistics
import sys

try:
    from openai import AsyncOpenAI
except ImportError:
    print("é”™è¯¯: éœ€è¦å®‰è£… openai åŒ…")
    print("è¿è¡Œ: pip install openai")
    sys.exit(1)


@dataclass
class RequestMetrics:
    """å•ä¸ªè¯·æ±‚çš„æ€§èƒ½æŒ‡æ ‡"""
    request_id: int
    scenario: str
    round_num: int

    # åŸºæœ¬ä¿¡æ¯
    prompt_length: int
    output_length: int

    # æ—¶é—´æŒ‡æ ‡
    ttft: Optional[float] = None  # Prefill + KVä¼ è¾“ + é¦–tokenç”Ÿæˆ
    tpot: Optional[float] = None  # Decode é˜¶æ®µæ¯tokenæ—¶é—´
    e2e_latency: float = 0.0

    # PD åˆ†ç¦»ç‰¹æœ‰æŒ‡æ ‡
    prefill_time: Optional[float] = None  # Prefill é˜¶æ®µæ—¶é—´ï¼ˆä¼°ç®—ï¼‰
    kv_transfer_time: Optional[float] = None  # KV Cache ä¼ è¾“æ—¶é—´ï¼ˆä¼°ç®—ï¼‰
    decode_time: Optional[float] = None  # Decode é˜¶æ®µæ—¶é—´

    # çŠ¶æ€
    success: bool = True
    error: Optional[str] = None
    timestamp: float = 0.0


@dataclass
class ScenarioStats:
    """åœºæ™¯ç»Ÿè®¡æ•°æ®"""
    scenario: str
    round_num: int
    total_requests: int
    success_requests: int
    failed_requests: int
    total_time: float

    # TTFT ç»Ÿè®¡ï¼ˆåŒ…å« Prefill + KV ä¼ è¾“ï¼‰
    avg_ttft: float = 0.0
    median_ttft: float = 0.0
    p90_ttft: float = 0.0
    p99_ttft: float = 0.0

    # TPOT ç»Ÿè®¡ï¼ˆDecode é˜¶æ®µï¼‰
    avg_tpot: float = 0.0
    median_tpot: float = 0.0
    p90_tpot: float = 0.0

    # å»¶è¿Ÿç»Ÿè®¡
    avg_latency: float = 0.0
    median_latency: float = 0.0
    p90_latency: float = 0.0
    p99_latency: float = 0.0

    # ååé‡
    request_throughput: float = 0.0
    token_throughput: float = 0.0
    output_token_throughput: float = 0.0

    # PD åˆ†ç¦»ç‰¹æœ‰ç»Ÿè®¡
    avg_prefill_time: float = 0.0
    avg_decode_time: float = 0.0
    cache_hit_rate: float = 0.0  # ä¼°ç®—çš„ç¼“å­˜å‘½ä¸­ç‡


class PDDisaggTestRunner:
    """PD åˆ†ç¦»æ¨¡å¼æµ‹è¯•è¿è¡Œå™¨"""

    def __init__(self, config_path: str = "test_config_large_models.yaml"):
        """åˆå§‹åŒ–æµ‹è¯•è¿è¡Œå™¨"""
        with open(config_path, 'r', encoding='utf-8') as f:
            self.config = yaml.safe_load(f)

        # è·å–æ´»åŠ¨æ¨¡å‹é…ç½®
        active_model = self.config['active_model']
        self.model_config = self.config['models'][active_model]
        self.model_name = self.model_config['name']

        # è·å– PD åˆ†ç¦»éƒ¨ç½²é…ç½®
        deploy_config = self.config['deployment_modes']['pd_disaggregated']
        if not deploy_config['enabled']:
            raise ValueError("PD åˆ†ç¦»æ¨¡å¼æœªå¯ç”¨ï¼Œè¯·æ£€æŸ¥é…ç½®æ–‡ä»¶")

        proxy_config = deploy_config['proxy']
        self.proxy_url = f"http://{proxy_config['host']}:{proxy_config['port']}/v1"

        self.results: List[RequestMetrics] = []
        self.scenario_stats: List[ScenarioStats] = []

        print(f"âœ“ PD åˆ†ç¦»æ¨¡å¼æµ‹è¯•é…ç½®åŠ è½½æˆåŠŸ")
        print(f"  æ¨¡å‹: {self.model_name}")
        print(f"  Proxy: {self.proxy_url}")
        print(f"  Prefiller: {deploy_config['prefiller']['host']}:{deploy_config['prefiller']['port']}")
        print(f"  Decoder: {deploy_config['decoder']['host']}:{deploy_config['decoder']['port']}")

    def _generate_long_document(self, length: int = 16384) -> str:
        """ç”ŸæˆæŒ‡å®šé•¿åº¦çš„é•¿æ–‡æ¡£"""
        base_text = """
        # Distributed Machine Learning Systems: A Comprehensive Guide

        ## 1. Introduction to Distributed Training
        Deep learning models have grown exponentially in size and complexity. Training these massive
        models requires distributed computing across multiple GPUs and nodes. This guide explores
        the fundamental concepts, architectures, and best practices for distributed machine learning.

        ## 2. Data Parallelism
        Data parallelism is the most common approach where different GPUs process different batches
        of data with the same model. Each GPU maintains a complete copy of the model and computes
        gradients independently. These gradients are then aggregated using collective communication
        operations like AllReduce.

        ### 2.1 Synchronous Data Parallelism
        In synchronous data parallelism, all workers wait for gradient aggregation before updating
        model parameters. This ensures consistency but may suffer from stragglers.

        ### 2.2 Asynchronous Data Parallelism
        Asynchronous approaches allow workers to update independently, improving throughput but
        potentially affecting convergence due to stale gradients.

        ## 3. Model Parallelism
        When models are too large to fit in a single GPU, model parallelism distributes different
        parts of the model across multiple devices. This includes:

        ### 3.1 Tensor Parallelism
        Splits individual layers across devices, requiring intensive communication between GPUs.

        ### 3.2 Pipeline Parallelism
        Divides the model into stages, with each stage assigned to different GPUs. Micro-batching
        is used to improve efficiency.

        ## 4. Communication Optimization
        Efficient communication is critical for distributed training performance:
        - Gradient compression and quantization
        - Overlapping communication with computation
        - Hierarchical AllReduce algorithms
        - RDMA and NVLink for high-bandwidth interconnects

        ## 5. Fault Tolerance and Checkpointing
        Distributed systems must handle failures gracefully through:
        - Regular checkpointing of model states
        - Elastic training with dynamic worker pools
        - Automatic failure detection and recovery

        ## 6. Performance Optimization
        Key optimization techniques include:
        - Mixed precision training (FP16/BF16)
        - Gradient accumulation
        - Zero Redundancy Optimizer (ZeRO)
        - FlashAttention and memory-efficient attention

        ## 7. Case Studies

        ### 7.1 Training GPT-3
        GPT-3 with 175B parameters required innovative approaches combining data, tensor, and
        pipeline parallelism across thousands of GPUs.

        ### 7.2 Training Stable Diffusion
        Diffusion models present unique challenges with their iterative denoising process and
        attention mechanisms operating on high-resolution feature maps.

        ## 8. Future Directions
        Emerging trends in distributed ML:
        - Disaggregated architectures separating compute and memory
        - Mixture-of-Experts (MoE) scaling
        - Cross-datacenter training
        - Energy-efficient training methods
        """

        words_needed = int(length / 1.3)
        repeated_text = (base_text * (words_needed // len(base_text.split()) + 1))
        words = repeated_text.split()[:words_needed]

        return ' '.join(words)

    def generate_prompts(self, scenario: str) -> List[Dict[str, Any]]:
        """ç”Ÿæˆæµ‹è¯•æç¤ºè¯"""
        scenario_config = self.config['test_scenarios'][scenario]
        num_requests = scenario_config['num_requests']

        prompts = []

        if scenario == 'long_context_high_reuse':
            long_doc = self._generate_long_document(scenario_config['context_length'])
            questions = scenario_config['questions']

            for i in range(num_requests):
                question = questions[i % len(questions)]
                prompt = f"{long_doc}\n\nåŸºäºä¸Šè¿°æŠ€æœ¯æ–‡æ¡£ï¼Œè¯·è¯¦ç»†å›ç­”é—®é¢˜ #{i}: {question}"
                prompts.append({
                    'prompt': prompt,
                    'estimated_tokens': scenario_config['context_length']
                })

        elif scenario == 'code_generation':
            code_context = self._generate_long_document(8192)
            tasks = scenario_config['tasks']

            for i in range(num_requests):
                task = tasks[i % len(tasks)]
                prompt = f"ä»£ç åº“ä¸Šä¸‹æ–‡ï¼š\n{code_context}\n\nä»»åŠ¡ #{i}: {task}\n\nè¯·æä¾›è¯¦ç»†çš„å®ç°æ–¹æ¡ˆå’Œä»£ç ã€‚"
                prompts.append({
                    'prompt': prompt,
                    'estimated_tokens': 8192
                })

        elif scenario == 'multi_turn_conversation':
            conversation_history = ""
            turns = scenario_config['conversation_turns']

            for i in range(num_requests):
                turn = turns[i % len(turns)]
                conversation_history += f"\n\nUser: {turn}\nAssistant: è¿™æ˜¯ä¸€ä¸ªè¯¦ç»†çš„å›ç­”ï¼Œè§£é‡Šäº†ç›¸å…³æ¦‚å¿µã€åŸç†å’Œåº”ç”¨åœºæ™¯ã€‚\n"

                prompt = f"ä»¥ä¸‹æ˜¯å¯¹è¯å†å²ï¼š{conversation_history}\n\nUser: {turn}\nAssistant:"
                prompts.append({
                    'prompt': prompt,
                    'estimated_tokens': len(conversation_history.split()) * 1.3
                })

        elif scenario == 'batch_processing':
            instruction = "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šç¿»è¯‘åŠ©æ‰‹ã€‚è¯·å°†ä»¥ä¸‹è‹±æ–‡å‡†ç¡®ç¿»è¯‘æˆä¸­æ–‡ï¼š\n" * 100  # é•¿æŒ‡ä»¤æ¨¡æ¿

            for i in range(num_requests):
                text = f"Technical document {i}: Distributed systems enable scalable computing..."
                prompt = f"{instruction}\n\nText #{i}:\n{text}"
                prompts.append({
                    'prompt': prompt,
                    'estimated_tokens': 1500
                })

        elif scenario == 'cold_start':
            for i in range(num_requests):
                unique_content = self._generate_long_document(2000)
                prompt = f"ç‹¬ç‰¹é—®é¢˜ #{i * 137}:\n{unique_content}\n\nè¯·åˆ†æè¿™ä¸ªé—®é¢˜ã€‚"
                prompts.append({
                    'prompt': prompt,
                    'estimated_tokens': 2000
                })

        else:
            raise ValueError(f"æœªçŸ¥çš„æµ‹è¯•åœºæ™¯: {scenario}")

        return prompts

    async def send_request(
        self,
        client: AsyncOpenAI,
        prompt_data: Dict[str, Any],
        request_id: int,
        scenario: str,
        round_num: int
    ) -> RequestMetrics:
        """å‘é€å•ä¸ªè¯·æ±‚å¹¶æµ‹é‡æ€§èƒ½æŒ‡æ ‡"""
        prompt = prompt_data['prompt']
        estimated_tokens = prompt_data['estimated_tokens']

        start_time = time.time()
        timestamp = datetime.now().timestamp()

        try:
            first_token_time = None
            output_tokens = 0

            stream = await client.chat.completions.create(
                model=self.model_name,
                messages=[{"role": "user", "content": prompt}],
                max_tokens=self.model_config['max_tokens'],
                temperature=self.model_config['temperature'],
                stream=True,
            )

            async for chunk in stream:
                if first_token_time is None:
                    first_token_time = time.time()

                if chunk.choices and chunk.choices[0].delta.content:
                    output_tokens += 1

            end_time = time.time()

            # è®¡ç®—æŒ‡æ ‡
            e2e_latency = end_time - start_time
            ttft = (first_token_time - start_time) if first_token_time else None

            # ä¼°ç®— PD åˆ†ç¦»é˜¶æ®µæ—¶é—´
            # TTFT = Prefill + KV Transfer + First Decode
            # å‡è®¾ Prefill â‰ˆ 70% of TTFT, KV Transfer â‰ˆ 20%, First Decode â‰ˆ 10%
            prefill_time = ttft * 0.7 if ttft else None
            kv_transfer_time = ttft * 0.2 if ttft else None

            total_output_time = end_time - first_token_time if first_token_time else e2e_latency
            decode_time = total_output_time
            tpot = (total_output_time / output_tokens) if output_tokens > 0 else None

            return RequestMetrics(
                request_id=request_id,
                scenario=scenario,
                round_num=round_num,
                prompt_length=int(estimated_tokens),
                output_length=output_tokens,
                ttft=ttft,
                tpot=tpot,
                e2e_latency=e2e_latency,
                prefill_time=prefill_time,
                kv_transfer_time=kv_transfer_time,
                decode_time=decode_time,
                success=True,
                timestamp=timestamp
            )

        except Exception as e:
            end_time = time.time()
            return RequestMetrics(
                request_id=request_id,
                scenario=scenario,
                round_num=round_num,
                prompt_length=int(estimated_tokens),
                output_length=0,
                e2e_latency=end_time - start_time,
                success=False,
                error=str(e),
                timestamp=timestamp
            )

    async def run_scenario(
        self,
        scenario: str,
        num_rounds: int = 2,
        concurrency: Optional[int] = None
    ) -> List[ScenarioStats]:
        """è¿è¡Œå•ä¸ªæµ‹è¯•åœºæ™¯"""
        print(f"\n{'='*80}")
        print(f"ğŸ§ª PD åˆ†ç¦»æµ‹è¯•åœºæ™¯: {scenario}")
        print(f"   {self.config['test_scenarios'][scenario]['description']}")
        print(f"{'='*80}")

        client = AsyncOpenAI(base_url=self.proxy_url, api_key="dummy")
        prompts = self.generate_prompts(scenario)

        print(f"ç”Ÿæˆäº† {len(prompts)} ä¸ªæµ‹è¯•è¯·æ±‚")
        print(f"å°†è¿è¡Œ {num_rounds} è½®æµ‹è¯•")

        scenario_stats = []

        for round_num in range(num_rounds):
            print(f"\n{'â”€'*80}")
            print(f"ğŸ“Š Round {round_num + 1}/{num_rounds}")
            if round_num == 0:
                print("   ğŸ¥¶ Cold Start - KV Cache å†·å¯åŠ¨")
            else:
                print("   ğŸ”¥ Cache Hit - æµ‹è¯•ç¼“å­˜æ•ˆæœ")
            print(f"{'â”€'*80}")

            round_start = time.time()

            tasks = [
                self.send_request(client, prompt, i, scenario, round_num + 1)
                for i, prompt in enumerate(prompts)
            ]

            if concurrency:
                results = []
                for i in range(0, len(tasks), concurrency):
                    batch = tasks[i:i+concurrency]
                    batch_results = await asyncio.gather(*batch)
                    results.extend(batch_results)
                    print(f"  å·²å®Œæˆ {min(i+concurrency, len(tasks))}/{len(tasks)} ä¸ªè¯·æ±‚...")
            else:
                results = await asyncio.gather(*tasks)

            round_elapsed = time.time() - round_start

            self.results.extend(results)

            stats = self._calculate_stats(results, scenario, round_num + 1, round_elapsed)
            scenario_stats.append(stats)
            self.scenario_stats.append(stats)

            self._print_stats(stats, round_num)

            if round_num > 0:
                self._print_improvement(scenario_stats[0], stats)

            if round_num < num_rounds - 1:
                wait_time = 5
                print(f"\nç­‰å¾… {wait_time} ç§’åå¼€å§‹ä¸‹ä¸€è½®...")
                await asyncio.sleep(wait_time)

        return scenario_stats

    def _calculate_stats(
        self,
        results: List[RequestMetrics],
        scenario: str,
        round_num: int,
        total_time: float
    ) -> ScenarioStats:
        """è®¡ç®—ç»Ÿè®¡æ•°æ®"""
        success_results = [r for r in results if r.success]

        if not success_results:
            return ScenarioStats(
                scenario=scenario,
                round_num=round_num,
                total_requests=len(results),
                success_requests=0,
                failed_requests=len(results),
                total_time=total_time
            )

        ttfts = [r.ttft for r in success_results if r.ttft is not None]
        ttfts.sort()

        tpots = [r.tpot for r in success_results if r.tpot is not None]
        tpots.sort()

        latencies = [r.e2e_latency for r in success_results]
        latencies.sort()

        prefill_times = [r.prefill_time for r in success_results if r.prefill_time is not None]
        decode_times = [r.decode_time for r in success_results if r.decode_time is not None]

        total_input_tokens = sum(r.prompt_length for r in success_results)
        total_output_tokens = sum(r.output_length for r in success_results)
        total_tokens = total_input_tokens + total_output_tokens

        return ScenarioStats(
            scenario=scenario,
            round_num=round_num,
            total_requests=len(results),
            success_requests=len(success_results),
            failed_requests=len(results) - len(success_results),
            total_time=total_time,

            avg_ttft=statistics.mean(ttfts) if ttfts else 0,
            median_ttft=statistics.median(ttfts) if ttfts else 0,
            p90_ttft=ttfts[int(len(ttfts) * 0.9)] if ttfts else 0,
            p99_ttft=ttfts[int(len(ttfts) * 0.99)] if len(ttfts) > 1 else (ttfts[0] if ttfts else 0),

            avg_tpot=statistics.mean(tpots) if tpots else 0,
            median_tpot=statistics.median(tpots) if tpots else 0,
            p90_tpot=tpots[int(len(tpots) * 0.9)] if tpots else 0,

            avg_latency=statistics.mean(latencies),
            median_latency=statistics.median(latencies),
            p90_latency=latencies[int(len(latencies) * 0.9)],
            p99_latency=latencies[int(len(latencies) * 0.99)] if len(latencies) > 1 else latencies[0],

            request_throughput=len(success_results) / total_time if total_time > 0 else 0,
            token_throughput=total_tokens / total_time if total_time > 0 else 0,
            output_token_throughput=total_output_tokens / total_time if total_time > 0 else 0,

            avg_prefill_time=statistics.mean(prefill_times) if prefill_times else 0,
            avg_decode_time=statistics.mean(decode_times) if decode_times else 0,
        )

    def _print_stats(self, stats: ScenarioStats, round_num: int):
        """æ‰“å°ç»Ÿè®¡ç»“æœ"""
        print(f"\nğŸ“ˆ ç»Ÿè®¡ç»“æœ:")
        print(f"  æ€»è¯·æ±‚æ•°:         {stats.total_requests}")
        print(f"  æˆåŠŸè¯·æ±‚:         {stats.success_requests}")
        print(f"  å¤±è´¥è¯·æ±‚:         {stats.failed_requests}")
        print(f"  æ€»è€—æ—¶:           {stats.total_time:.2f}s")

        print(f"\nâ±ï¸  TTFT (Prefill + KV Transfer + First Token):")
        print(f"  å¹³å‡:             {stats.avg_ttft*1000:.2f}ms")
        print(f"  ä¸­ä½æ•°:           {stats.median_ttft*1000:.2f}ms")
        print(f"  P90:              {stats.p90_ttft*1000:.2f}ms")
        print(f"  P99:              {stats.p99_ttft*1000:.2f}ms")

        print(f"\nğŸ”„ PD åˆ†ç¦»é˜¶æ®µæ—¶é—´ï¼ˆä¼°ç®—ï¼‰:")
        print(f"  å¹³å‡ Prefill:     {stats.avg_prefill_time*1000:.2f}ms")
        print(f"  å¹³å‡ Decode:      {stats.avg_decode_time*1000:.2f}ms")

        print(f"\nâš¡ TPOT (Decode é˜¶æ®µ):")
        print(f"  å¹³å‡:             {stats.avg_tpot*1000:.2f}ms/token")
        print(f"  ä¸­ä½æ•°:           {stats.median_tpot*1000:.2f}ms/token")
        print(f"  P90:              {stats.p90_tpot*1000:.2f}ms/token")

        print(f"\nğŸ• ç«¯åˆ°ç«¯å»¶è¿Ÿ:")
        print(f"  å¹³å‡:             {stats.avg_latency:.2f}s")
        print(f"  ä¸­ä½æ•°:           {stats.median_latency:.2f}s")
        print(f"  P90:              {stats.p90_latency:.2f}s")
        print(f"  P99:              {stats.p99_latency:.2f}s")

        print(f"\nğŸš€ ååé‡:")
        print(f"  è¯·æ±‚ååé‡:       {stats.request_throughput:.2f} req/s")
        print(f"  Token ååé‡:     {stats.token_throughput:.2f} tokens/s")
        print(f"  è¾“å‡º Token:       {stats.output_token_throughput:.2f} tokens/s")

    def _print_improvement(self, baseline: ScenarioStats, current: ScenarioStats):
        """æ‰“å°ç¼“å­˜æ•ˆæœ"""
        print(f"\nğŸ¯ PD åˆ†ç¦» + ç¼“å­˜æ•ˆæœ (ç›¸æ¯” Round 1):")

        if baseline.avg_ttft > 0:
            ttft_reduction = (1 - current.avg_ttft / baseline.avg_ttft) * 100
            print(f"  âš¡ TTFT é™ä½:         {ttft_reduction:+.1f}%")

        if baseline.avg_prefill_time > 0:
            prefill_reduction = (1 - current.avg_prefill_time / baseline.avg_prefill_time) * 100
            print(f"  ğŸ”„ Prefill æ—¶é—´é™ä½:  {prefill_reduction:+.1f}%")

        if baseline.avg_latency > 0:
            latency_reduction = (1 - current.avg_latency / baseline.avg_latency) * 100
            print(f"  ğŸ“‰ å»¶è¿Ÿé™ä½:         {latency_reduction:+.1f}%")

        if baseline.request_throughput > 0:
            throughput_increase = (current.request_throughput / baseline.request_throughput - 1) * 100
            print(f"  ğŸ“ˆ ååé‡æå‡:       {throughput_increase:+.1f}%")

        # è¯„ä»·ç¼“å­˜æ•ˆæœ
        targets = self.config['performance_test']['targets']['pd_disaggregated']

        if baseline.avg_ttft > 0:
            if ttft_reduction >= targets['cache_hit_ttft_reduction']:
                print(f"\n  âœ… è¾¾åˆ°æ€§èƒ½ç›®æ ‡ï¼TTFT é™ä½ {ttft_reduction:.1f}% (ç›®æ ‡: {targets['cache_hit_ttft_reduction']}%)")
            else:
                print(f"\n  âš ï¸  æœªè¾¾åˆ°ç›®æ ‡ã€‚TTFT é™ä½ {ttft_reduction:.1f}% (ç›®æ ‡: {targets['cache_hit_ttft_reduction']}%)")

        if baseline.request_throughput > 0:
            if throughput_increase >= targets['cache_hit_throughput_increase']:
                print(f"  âœ… è¾¾åˆ°æ€§èƒ½ç›®æ ‡ï¼ååé‡æå‡ {throughput_increase:.1f}% (ç›®æ ‡: {targets['cache_hit_throughput_increase']}%)")
            else:
                print(f"  âš ï¸  æœªè¾¾åˆ°ç›®æ ‡ã€‚ååé‡æå‡ {throughput_increase:.1f}% (ç›®æ ‡: {targets['cache_hit_throughput_increase']}%)")

    def save_results(self, output_dir: str = "test_results"):
        """ä¿å­˜æµ‹è¯•ç»“æœ"""
        import os
        os.makedirs(output_dir, exist_ok=True)

        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        model_name = self.config['active_model']

        results_file = f"{output_dir}/pd_disagg_{model_name}_results_{timestamp}.json"
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump([asdict(r) for r in self.results], f, indent=2, ensure_ascii=False)

        stats_file = f"{output_dir}/pd_disagg_{model_name}_stats_{timestamp}.json"
        with open(stats_file, 'w', encoding='utf-8') as f:
            json.dump([asdict(s) for s in self.scenario_stats], f, indent=2, ensure_ascii=False)

        print(f"\nğŸ’¾ ç»“æœå·²ä¿å­˜:")
        print(f"  è¯¦ç»†ç»“æœ: {results_file}")
        print(f"  ç»Ÿè®¡æ‘˜è¦: {stats_file}")

        return results_file, stats_file


async def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="PD åˆ†ç¦»æ¨¡å¼ç¼“å­˜æµ‹è¯•")
    parser.add_argument('--config', type=str, default='test_config_large_models.yaml')
    parser.add_argument('--scenarios', type=str, nargs='+',
                       default=['long_context_high_reuse', 'multi_turn_conversation'])
    parser.add_argument('--rounds', type=int, default=2)
    parser.add_argument('--concurrency', type=int, help='å¹¶å‘é™åˆ¶')
    parser.add_argument('--output-dir', type=str, default='test_results')

    args = parser.parse_args()

    print("="*80)
    print("ğŸš€ vLLM PD åˆ†ç¦»æ¨¡å¼ + LMCache + Mooncake ç¼“å­˜æµ‹è¯•")
    print("="*80)
    print(f"å¼€å§‹æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")

    try:
        runner = PDDisaggTestRunner(args.config)

        for scenario in args.scenarios:
            await runner.run_scenario(scenario, args.rounds, args.concurrency)

        runner.save_results(args.output_dir)

        print(f"\n{'='*80}")
        print("âœ… PD åˆ†ç¦»æµ‹è¯•å®Œæˆ!")
        print(f"{'='*80}")

    except Exception as e:
        print(f"\nâŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    asyncio.run(main())
