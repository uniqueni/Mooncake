#!/usr/bin/env python3
"""
ç®€åŒ–æµ‹è¯•è„šæœ¬ - ç›´æ¥è¿æ¥åˆ° OpenAI å…¼å®¹æ¥å£

é€‚ç”¨äºå·²æœ‰ Mooncake å’Œ OpenAI æ¥å£çš„åœºæ™¯ã€‚
"""

import asyncio
import json
import time
import argparse
import yaml
from typing import List, Dict, Any, Optional
from dataclasses import dataclass, asdict
from datetime import datetime
import statistics
import sys

try:
    from openai import AsyncOpenAI
except ImportError:
    print("é”™è¯¯: éœ€è¦å®‰è£… openai åŒ…")
    print("è¿è¡Œ: pip install openai")
    sys.exit(1)


@dataclass
class RequestMetrics:
    """è¯·æ±‚æŒ‡æ ‡"""
    request_id: int
    scenario: str
    round_num: int
    mode: str  # "with-cache" æˆ– "baseline"

    prompt_length: int
    output_length: int

    ttft: Optional[float] = None
    tpot: Optional[float] = None
    e2e_latency: float = 0.0

    success: bool = True
    error: Optional[str] = None
    timestamp: float = 0.0


@dataclass
class RoundStats:
    """è½®æ¬¡ç»Ÿè®¡"""
    scenario: str
    round_num: int
    mode: str
    total_requests: int
    success_requests: int
    total_time: float

    avg_ttft: float = 0.0
    median_ttft: float = 0.0
    p90_ttft: float = 0.0
    p99_ttft: float = 0.0

    avg_tpot: float = 0.0
    median_tpot: float = 0.0

    avg_latency: float = 0.0
    p90_latency: float = 0.0

    request_throughput: float = 0.0
    token_throughput: float = 0.0

    # è·¨èŠ‚ç‚¹æµ‹è¯•ä¿¡æ¯
    endpoint_url: Optional[str] = None  # ä½¿ç”¨çš„ endpoint URL


class SimpleTestRunner:
    """ç®€åŒ–æµ‹è¯•è¿è¡Œå™¨"""

    def __init__(self, config_path: str, mode: str = "with-cache"):
        """åˆå§‹åŒ–"""
        with open(config_path, 'r', encoding='utf-8') as f:
            self.config = yaml.safe_load(f)

        self.mode = mode  # "with-cache" æˆ– "baseline"
        self.api_url = self.config['openai_api']['base_url']
        self.model_name = self.config['openai_api']['model_name']
        self.api_key = self.config['openai_api']['api_key']

        # æ”¯æŒè·¨èŠ‚ç‚¹æµ‹è¯•ï¼šæ¯è½®ä½¿ç”¨ä¸åŒçš„ endpoint
        self.endpoints_per_round = self.config['openai_api'].get('endpoints_per_round', {})

        self.results: List[RequestMetrics] = []
        self.round_stats: List[RoundStats] = []

        print(f"âœ“ æµ‹è¯•é…ç½®åŠ è½½æˆåŠŸ")
        print(f"  æ¨¡å¼: {self.mode}")
        print(f"  API URL: {self.api_url}")
        print(f"  æ¨¡å‹: {self.model_name}")

        if self.endpoints_per_round:
            print(f"  è·¨èŠ‚ç‚¹æµ‹è¯•: å¯ç”¨")
            for round_num, endpoint in self.endpoints_per_round.items():
                print(f"    Round {round_num}: {endpoint}")

    def _generate_long_doc(self, length: int = 16384) -> str:
        """ç”Ÿæˆé•¿æ–‡æ¡£"""
        base = """
        # åˆ†å¸ƒå¼æœºå™¨å­¦ä¹ ç³»ç»Ÿç»¼è¿°

        ## 1. å¼•è¨€
        éšç€æ·±åº¦å­¦ä¹ æ¨¡å‹è§„æ¨¡çš„æŒ‡æ•°çº§å¢é•¿ï¼Œå•æœºè®­ç»ƒå·²æ— æ³•æ»¡è¶³éœ€æ±‚ã€‚åˆ†å¸ƒå¼è®­ç»ƒæˆä¸ºè®­ç»ƒ
        å¤§è§„æ¨¡æ¨¡å‹çš„å¿…è¦æ‰‹æ®µã€‚æœ¬æ–‡ä»‹ç»åˆ†å¸ƒå¼æœºå™¨å­¦ä¹ çš„æ ¸å¿ƒæ¦‚å¿µã€æ¶æ„è®¾è®¡å’Œæœ€ä½³å®è·µã€‚

        ## 2. æ•°æ®å¹¶è¡Œ
        æ•°æ®å¹¶è¡Œæ˜¯æœ€å¸¸è§çš„åˆ†å¸ƒå¼è®­ç»ƒæ–¹å¼ã€‚ä¸åŒGPUå¤„ç†ä¸åŒæ‰¹æ¬¡çš„æ•°æ®ï¼Œæ¯ä¸ªGPUç»´æŠ¤å®Œæ•´
        çš„æ¨¡å‹å‰¯æœ¬ï¼Œç‹¬ç«‹è®¡ç®—æ¢¯åº¦ã€‚æ¢¯åº¦é€šè¿‡AllReduceç­‰é›†åˆé€šä¿¡æ“ä½œè¿›è¡Œèšåˆã€‚

        ### 2.1 åŒæ­¥æ•°æ®å¹¶è¡Œ
        åŒæ­¥æ•°æ®å¹¶è¡Œä¸­ï¼Œæ‰€æœ‰workerç­‰å¾…æ¢¯åº¦èšåˆåæ‰æ›´æ–°å‚æ•°ã€‚è¿™ç¡®ä¿äº†ä¸€è‡´æ€§ï¼Œä½†å¯èƒ½
        å—åˆ°stragglersçš„å½±å“ã€‚

        ### 2.2 å¼‚æ­¥æ•°æ®å¹¶è¡Œ
        å¼‚æ­¥æ–¹å¼å…è®¸workerç‹¬ç«‹æ›´æ–°ï¼Œæé«˜ååé‡ä½†å¯èƒ½å› æ¢¯åº¦è¿‡æ—¶å½±å“æ”¶æ•›ã€‚

        ## 3. æ¨¡å‹å¹¶è¡Œ
        å½“æ¨¡å‹å¤ªå¤§æ— æ³•æ”¾å…¥å•ä¸ªGPUæ—¶ï¼Œæ¨¡å‹å¹¶è¡Œå°†æ¨¡å‹çš„ä¸åŒéƒ¨åˆ†åˆ†å¸ƒåˆ°å¤šä¸ªè®¾å¤‡ä¸Šã€‚

        ### 3.1 å¼ é‡å¹¶è¡Œ
        å°†å•ä¸ªå±‚åˆ†å‰²åˆ°å¤šä¸ªè®¾å¤‡ï¼Œéœ€è¦è®¾å¤‡é—´å¯†é›†é€šä¿¡ã€‚

        ### 3.2 æµæ°´çº¿å¹¶è¡Œ
        å°†æ¨¡å‹åˆ†ä¸ºå¤šä¸ªé˜¶æ®µï¼Œæ¯ä¸ªé˜¶æ®µåˆ†é…åˆ°ä¸åŒGPUã€‚ä½¿ç”¨micro-batchingæé«˜æ•ˆç‡ã€‚

        ## 4. é€šä¿¡ä¼˜åŒ–
        é«˜æ•ˆé€šä¿¡æ˜¯åˆ†å¸ƒå¼è®­ç»ƒæ€§èƒ½çš„å…³é”®ï¼š
        - æ¢¯åº¦å‹ç¼©å’Œé‡åŒ–
        - è®¡ç®—ä¸é€šä¿¡é‡å 
        - åˆ†å±‚AllReduceç®—æ³•
        - RDMAå’ŒNVLinké«˜å¸¦å®½äº’è¿

        ## 5. å®¹é”™ä¸æ£€æŸ¥ç‚¹
        åˆ†å¸ƒå¼ç³»ç»Ÿå¿…é¡»ä¼˜é›…å¤„ç†æ•…éšœï¼š
        - å®šæœŸä¿å­˜æ¨¡å‹çŠ¶æ€
        - å¼¹æ€§è®­ç»ƒä¸åŠ¨æ€workeræ± 
        - è‡ªåŠ¨æ•…éšœæ£€æµ‹å’Œæ¢å¤

        ## 6. æ€§èƒ½ä¼˜åŒ–
        å…³é”®ä¼˜åŒ–æŠ€æœ¯ï¼š
        - æ··åˆç²¾åº¦è®­ç»ƒï¼ˆFP16/BF16ï¼‰
        - æ¢¯åº¦ç´¯ç§¯
        - Zero Redundancy Optimizerï¼ˆZeROï¼‰
        - FlashAttentionå’Œå†…å­˜é«˜æ•ˆæ³¨æ„åŠ›

        ## 7. æ¡ˆä¾‹ç ”ç©¶
        GPT-3ï¼ˆ175Bå‚æ•°ï¼‰çš„è®­ç»ƒéœ€è¦åˆ›æ–°çš„æ•°æ®ã€å¼ é‡å’Œæµæ°´çº¿å¹¶è¡Œç»„åˆï¼Œæ¨ªè·¨æ•°åƒGPUã€‚
        """

        words_needed = int(length / 1.3)
        repeated = (base * (words_needed // len(base.split()) + 1))
        return ' '.join(repeated.split()[:words_needed])

    def generate_prompts(self, scenario: str) -> List[Dict[str, Any]]:
        """ç”Ÿæˆæµ‹è¯•æç¤ºè¯"""
        config = self.config['test_scenarios'][scenario]
        num_requests = config['num_requests']

        prompts = []

        if scenario == 'long_context_high_reuse':
            doc = self._generate_long_doc(config['context_length'])
            questions = config['questions']

            for i in range(num_requests):
                q = questions[i % len(questions)]
                prompt = f"{doc}\n\né—®é¢˜ #{i}: {q}"
                prompts.append({
                    'prompt': prompt,
                    'tokens': config['context_length']
                })

        elif scenario == 'multi_turn_conversation':
            history = ""
            turns = config['conversation_turns']

            for i in range(num_requests):
                turn = turns[i % len(turns)]
                history += f"\n\nUser: {turn}\nAssistant: [è¯¦ç»†å›ç­”]\n"
                prompt = f"å¯¹è¯å†å²ï¼š{history}\n\nUser: {turn}\nAssistant:"
                prompts.append({
                    'prompt': prompt,
                    'tokens': len(history.split()) * 1.3
                })

        elif scenario == 'code_generation':
            code_ctx = self._generate_long_doc(config['context_length'])
            tasks = config['tasks']

            for i in range(num_requests):
                task = tasks[i % len(tasks)]
                prompt = f"ä»£ç åº“ï¼š\n{code_ctx}\n\nä»»åŠ¡ #{i}: {task}"
                prompts.append({
                    'prompt': prompt,
                    'tokens': config['context_length']
                })

        elif scenario == 'batch_processing':
            instruction = "ä½ æ˜¯ä¸“ä¸šç¿»è¯‘ã€‚" * int(config['instruction_length'] / 10)

            for i in range(num_requests):
                text = f"Technical text {i}: Distributed systems..."
                prompt = f"{instruction}\n\næ–‡æœ¬ #{i}:\n{text}"
                prompts.append({
                    'prompt': prompt,
                    'tokens': config['instruction_length']
                })

        elif scenario == 'cold_start':
            for i in range(num_requests):
                content = self._generate_long_doc(config['content_length'])
                prompt = f"é—®é¢˜ #{i * 137}:\n{content}\n\nè¯·åˆ†æã€‚"
                prompts.append({
                    'prompt': prompt,
                    'tokens': config['content_length']
                })

        return prompts

    async def send_request(
        self,
        client: AsyncOpenAI,
        prompt_data: Dict,
        request_id: int,
        scenario: str,
        round_num: int
    ) -> RequestMetrics:
        """å‘é€è¯·æ±‚"""
        prompt = prompt_data['prompt']
        est_tokens = prompt_data['tokens']

        start = time.time()
        first_token_time = None
        output_tokens = 0

        try:
            stream = await client.chat.completions.create(
                model=self.model_name,
                messages=[{"role": "user", "content": prompt}],
                max_tokens=self.config['model']['max_tokens'],
                temperature=self.config['model']['temperature'],
                stream=True,
            )

            async for chunk in stream:
                if first_token_time is None:
                    first_token_time = time.time()
                if chunk.choices and chunk.choices[0].delta.content:
                    output_tokens += 1

            end = time.time()

            ttft = (first_token_time - start) if first_token_time else None
            e2e = end - start
            tpot = ((end - first_token_time) / output_tokens) if first_token_time and output_tokens > 0 else None

            return RequestMetrics(
                request_id=request_id,
                scenario=scenario,
                round_num=round_num,
                mode=self.mode,
                prompt_length=int(est_tokens),
                output_length=output_tokens,
                ttft=ttft,
                tpot=tpot,
                e2e_latency=e2e,
                success=True,
                timestamp=datetime.now().timestamp()
            )

        except Exception as e:
            return RequestMetrics(
                request_id=request_id,
                scenario=scenario,
                round_num=round_num,
                mode=self.mode,
                prompt_length=int(est_tokens),
                output_length=0,
                e2e_latency=time.time() - start,
                success=False,
                error=str(e),
                timestamp=datetime.now().timestamp()
            )

    async def run_scenario(
        self,
        scenario: str,
        num_rounds: int = 2,
        concurrency: Optional[int] = None
    ):
        """è¿è¡Œæµ‹è¯•åœºæ™¯"""
        print(f"\n{'='*80}")
        print(f"ğŸ§ª åœºæ™¯: {scenario}")
        print(f"   {self.config['test_scenarios'][scenario]['description']}")
        print(f"{'='*80}")

        prompts = self.generate_prompts(scenario)
        print(f"ç”Ÿæˆ {len(prompts)} ä¸ªè¯·æ±‚ï¼Œè¿è¡Œ {num_rounds} è½®")

        for round_num in range(num_rounds):
            print(f"\n{'â”€'*80}")
            print(f"ğŸ“Š Round {round_num + 1}/{num_rounds}")
            if round_num == 0:
                print("   ğŸ¥¶ Cold Start")
            else:
                print("   ğŸ”¥ Cache Hit")

            # æ ¹æ®è½®æ¬¡é€‰æ‹© endpointï¼ˆæ”¯æŒè·¨èŠ‚ç‚¹æµ‹è¯•ï¼‰
            round_api_url = self.endpoints_per_round.get(round_num + 1, self.api_url)
            if round_api_url != self.api_url:
                print(f"   ğŸŒ è·¨èŠ‚ç‚¹æµ‹è¯• - ä½¿ç”¨ endpoint: {round_api_url}")
            else:
                print(f"   ğŸ–¥ï¸  ä½¿ç”¨ endpoint: {round_api_url}")

            print(f"{'â”€'*80}")

            # ä¸ºæ¯è½®åˆ›å»ºæ–°çš„ clientï¼ˆæ”¯æŒä¸åŒçš„ endpointï¼‰
            client = AsyncOpenAI(base_url=round_api_url, api_key=self.api_key)
            round_start = time.time()

            tasks = [
                self.send_request(client, p, i, scenario, round_num + 1)
                for i, p in enumerate(prompts)
            ]

            if concurrency:
                results = []
                for i in range(0, len(tasks), concurrency):
                    batch = await asyncio.gather(*tasks[i:i+concurrency])
                    results.extend(batch)
                    print(f"  å®Œæˆ {min(i+concurrency, len(tasks))}/{len(tasks)}")
            else:
                results = await asyncio.gather(*tasks)

            elapsed = time.time() - round_start

            self.results.extend(results)
            stats = self._calc_stats(results, scenario, round_num + 1, elapsed, round_api_url)
            self.round_stats.append(stats)

            self._print_stats(stats, round_num)

            if round_num > 0:
                self._print_improvement(self.round_stats[-2], stats)

            if round_num < num_rounds - 1:
                wait = self.config['test_execution'].get('wait_between_rounds', 15)
                print(f"\nç­‰å¾… {wait}s...")
                await asyncio.sleep(wait)

    def _calc_stats(self, results, scenario, round_num, total_time, endpoint_url=None) -> RoundStats:
        """è®¡ç®—ç»Ÿè®¡"""
        success = [r for r in results if r.success]

        if not success:
            return RoundStats(
                scenario=scenario,
                round_num=round_num,
                mode=self.mode,
                total_requests=len(results),
                success_requests=0,
                total_time=total_time,
                request_throughput=0,
                endpoint_url=endpoint_url
            )

        ttfts = sorted([r.ttft for r in success if r.ttft])
        tpots = sorted([r.tpot for r in success if r.tpot])
        latencies = sorted([r.e2e_latency for r in success])

        total_tokens = sum(r.prompt_length + r.output_length for r in success)

        return RoundStats(
            scenario=scenario,
            round_num=round_num,
            mode=self.mode,
            total_requests=len(results),
            success_requests=len(success),
            total_time=total_time,

            avg_ttft=statistics.mean(ttfts) if ttfts else 0,
            median_ttft=statistics.median(ttfts) if ttfts else 0,
            p90_ttft=ttfts[int(len(ttfts)*0.9)] if ttfts else 0,
            p99_ttft=ttfts[int(len(ttfts)*0.99)] if len(ttfts) > 1 else (ttfts[0] if ttfts else 0),

            avg_tpot=statistics.mean(tpots) if tpots else 0,
            median_tpot=statistics.median(tpots) if tpots else 0,

            avg_latency=statistics.mean(latencies),
            p90_latency=latencies[int(len(latencies)*0.9)],

            request_throughput=len(success) / total_time if total_time > 0 else 0,
            token_throughput=total_tokens / total_time if total_time > 0 else 0
        )

    def _print_stats(self, stats: RoundStats, round_num: int):
        """æ‰“å°ç»Ÿè®¡"""
        print(f"\nğŸ“ˆ ç»Ÿè®¡:")
        print(f"  è¯·æ±‚æ•°: {stats.success_requests}/{stats.total_requests}")
        print(f"  è€—æ—¶:   {stats.total_time:.2f}s")
        print(f"\nâ±ï¸  TTFT:")
        print(f"  å¹³å‡:   {stats.avg_ttft*1000:.2f}ms")
        print(f"  ä¸­ä½:   {stats.median_ttft*1000:.2f}ms")
        print(f"  P90:    {stats.p90_ttft*1000:.2f}ms")
        print(f"\nâš¡ TPOT:")
        print(f"  å¹³å‡:   {stats.avg_tpot*1000:.2f}ms/token")
        print(f"\nğŸš€ åå:")
        print(f"  è¯·æ±‚:   {stats.request_throughput:.2f} req/s")
        print(f"  Token:  {stats.token_throughput:.2f} tokens/s")

    def _print_improvement(self, baseline: RoundStats, current: RoundStats):
        """æ‰“å°æ”¹å–„"""
        print(f"\nğŸ¯ ç¼“å­˜æ•ˆæœ:")

        if baseline.avg_ttft > 0:
            ttft_imp = (1 - current.avg_ttft / baseline.avg_ttft) * 100
            print(f"  TTFT é™ä½:    {ttft_imp:+.1f}%")

        if baseline.request_throughput > 0:
            thr_imp = (current.request_throughput / baseline.request_throughput - 1) * 100
            print(f"  ååé‡æå‡:   {thr_imp:+.1f}%")

        targets = self.config['performance_targets']['cache_hit']

        if baseline.avg_ttft > 0:
            if ttft_imp >= targets['ttft_reduction_percent']:
                print(f"  âœ… è¾¾åˆ°ç›®æ ‡ï¼ï¼ˆ{targets['ttft_reduction_percent']}%ï¼‰")
            else:
                print(f"  âš ï¸  æœªè¾¾ç›®æ ‡ï¼ˆ{targets['ttft_reduction_percent']}%ï¼‰")

    def save_results(self, output_dir: str = "test_results"):
        """ä¿å­˜ç»“æœ"""
        import os
        os.makedirs(output_dir, exist_ok=True)

        ts = datetime.now().strftime("%Y%m%d_%H%M%S")
        model_size = self.config['model']['size']

        # è·å–åœºæ™¯åˆ—è¡¨ï¼Œç”Ÿæˆç®€çŸ­çš„åœºæ™¯æ ‡è¯†
        scenarios = list(set([r.scenario for r in self.results]))
        if len(scenarios) == 1:
            scenario_prefix = scenarios[0]
        else:
            scenario_prefix = f"{scenarios[0]}_etc"  # å¤šåœºæ™¯æ—¶åªæ˜¾ç¤ºç¬¬ä¸€ä¸ª

        # æ–‡ä»¶åæ ¼å¼: {mode}_{scenario}_{model_size}_results_{timestamp}.json
        results_file = f"{output_dir}/{self.mode}_{scenario_prefix}_{model_size}_results_{ts}.json"
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump([asdict(r) for r in self.results], f, indent=2)

        stats_file = f"{output_dir}/{self.mode}_{scenario_prefix}_{model_size}_stats_{ts}.json"
        with open(stats_file, 'w', encoding='utf-8') as f:
            json.dump([asdict(s) for s in self.round_stats], f, indent=2)

        print(f"\nğŸ’¾ ç»“æœå·²ä¿å­˜:")
        print(f"  {results_file}")
        print(f"  {stats_file}")

        return results_file, stats_file


async def main():
    parser = argparse.ArgumentParser(description="ç®€åŒ–æµ‹è¯•è„šæœ¬")
    parser.add_argument('--config', default='test_config_simple.yaml')
    parser.add_argument('--mode', choices=['with-cache', 'baseline'], default='with-cache')
    parser.add_argument('--scenarios', nargs='+', default=['long_context_high_reuse'])
    parser.add_argument('--rounds', type=int, default=2)
    parser.add_argument('--concurrency', type=int)
    parser.add_argument('--output-dir', default='test_results')

    args = parser.parse_args()

    print("="*80)
    print(f"ğŸš€ æµ‹è¯•æ¨¡å¼: {args.mode}")
    print("="*80)

    try:
        runner = SimpleTestRunner(args.config, args.mode)

        for scenario in args.scenarios:
            await runner.run_scenario(scenario, args.rounds, args.concurrency)

        runner.save_results(args.output_dir)

        print(f"\n{'='*80}")
        print("âœ… æµ‹è¯•å®Œæˆ!")
        print(f"{'='*80}")

    except Exception as e:
        print(f"\nâŒ å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    asyncio.run(main())
